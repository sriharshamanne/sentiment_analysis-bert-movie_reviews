{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data\n",
    "    \n",
    "Sentiment Analysis on Movie Reviews - kaggle \n",
    "\n",
    "using the Rotten Tomatoes movie reviews dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                      | 0.00/1.28M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train.tsv.zip to .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1.28M/1.28M [00:00<00:00, 6.89MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 494k/494k [00:00<00:00, 4.19MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading test.tsv.zip to .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "for file in ['train.tsv', 'test.tsv']:\n",
    "    api.competition_download_file('sentiment-analysis-on-movie-reviews', f'{file}.zip', path='./')\n",
    "\n",
    "    with zipfile.ZipFile(f'{file}.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "\n",
    "    os.remove(f'{file}.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are many copies through segments of the same answer - the SentenceId value for each of these copies is identical. We can remove these duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>157</td>\n",
       "      <td>5</td>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PhraseId  SentenceId                                             Phrase  \\\n",
       "0           1           1  A series of escapades demonstrating the adage ...   \n",
       "63         64           2  This quiet , introspective and entertaining in...   \n",
       "81         82           3  Even fans of Ismail Merchant 's work , I suspe...   \n",
       "116       117           4  A positively thrilling combination of ethnogra...   \n",
       "156       157           5  Aggressive self-glorification and a manipulati...   \n",
       "\n",
       "     Sentiment  \n",
       "0            1  \n",
       "63           4  \n",
       "81           1  \n",
       "116          3  \n",
       "156          1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=['SentenceId'], keep='first')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing this text to create two input tensors; our input IDs, and attention mask.\n",
    "\n",
    "have tensors within two numpy arrays, which will be of dimensions `len(df) * 512` - the `512` is the sequence length of our tokenized sequences for BERT, and `len(df)` the number of samples in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8529, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seq_len = 512\n",
    "num_samples = len(df)\n",
    "\n",
    "num_samples, seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing with a `BertTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# tokenize - Numpy tensors\n",
    "tokens = tokenizer(df['Phrase'].tolist(), max_length=seq_len, truncation=True,\n",
    "                   padding='max_length', add_special_tokens=True,\n",
    "                   return_tensors='np')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns three numpy arrays - *input_ids*, *token_type_ids*, and *attention_mask*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101,  138, 1326, ...,    0,    0,    0],\n",
       "       [ 101, 1188, 3589, ...,    0,    0,    0],\n",
       "       [ 101, 2431, 3899, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 1109,  146, ...,    0,    0,    0],\n",
       "       [ 101, 1252, 1122, ...,    0,    0,    0],\n",
       "       [ 101, 1192, 1180, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['attention_mask'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save them to file as Numpy binary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie-ids.npy', 'wb') as f:\n",
    "    np.save(f, tokens['input_ids'])\n",
    "with open('movie-mask.npy', 'wb') as f:\n",
    "    np.save(f, tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete the in-memory arrays to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target data is a set of integer values (representing sentiment classes) in the Sentiment column of our dataframe 'df'. We need to extract these values and one-hot encode them into another numpy array, which will have the dimensions 'len(df) * number of label classes'. Again, initialize a numpy zero array beforehand, populate using indexing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8529, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we then initialize the zero array\n",
    "labels = np.zeros((num_samples, arr.max()+1))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to use `arr.max()+1` to define our second dimension here because we have the values *\\[0, 1, 2, 3, 4\\]* in our *Sentiment* column, there are **5** unique labels which means we need our labels array to have five columns (one for each) - `arr.max() = 4`, so we do `4 + 1` to get our required value of `5`.\n",
    "\n",
    "Now we use the current values in our `arr` of *\\[0, 1, 2, 3, 4\\]* to place `1` values in the correct positions of our presently zeros-only array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.arange(num_samples), arr] = 1\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoded labels array - save this to file as a Numpy binary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie-labels.npy', 'wb') as f:\n",
    "    np.save(f, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('movie-ids.npy', 'rb') as f:\n",
    "    ids = np.load(f, allow_pickle=True)\n",
    "with open('movie-mask.npy', 'rb') as f:\n",
    "    mask = np.load(f, allow_pickle=True)\n",
    "with open('movie-labels.npy', 'rb') as f:\n",
    "    labels = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take these three arrays and create a TF dataset object with them using `from_tensor_slices` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((512,), (512,), (5,)), types: (tf.int32, tf.int32, tf.float64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((ids, mask, labels))\n",
    "\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in our dataset is a tuple containing a single `ids`, `mask`, and `labels` tensor. However, when feeding data into our model we need a two-item tuple in the format **(\\<inputs\\>, \\<outputs\\>)**. Now, we have two tensors for our inputs - so, what we do is enter our **\\<inputs\\>** tensor as a dictionary:\n",
    "\n",
    "```\n",
    "{\n",
    "    'input_ids': <input_id_tensor>,\n",
    "    'attention_mask': <mask_tensor>\n",
    "}\n",
    "```\n",
    "\n",
    "To rearrange the dataset format we `map` a function that modifies the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ({input_ids: (512,), attention_mask: (512,)}, (5,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.float64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    # convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "# use the dataset map method to apply this transformation\n",
    "dataset = dataset.map(map_func)\n",
    "\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, shuffle data, and batch it. take batch sizes of `16` and drop any samples that don't fit evenly into chunks of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16, 5)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.float64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 0.9\n",
    "\n",
    "# to calculate how many batches must be taken to create 90% training set\n",
    "size = int((ids.shape[0] / batch_size) * split)\n",
    "\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.take(size)\n",
    "val_ds = dataset.skip(size)\n",
    "\n",
    "# free up memory\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets are prepared for our model inputs. Now, save both to file using [`tf.data.experimental.save`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(train_ds, 'train')\n",
    "tf.data.experimental.save(val_ds, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while loading these files using `tf.data.experimental.load` need to define the tensor `element_spec` - which describes the tensor shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': TensorSpec(shape=(16, 512), dtype=tf.int32, name=None),\n",
       "  'attention_mask': TensorSpec(shape=(16, 512), dtype=tf.int32, name=None)},\n",
       " TensorSpec(shape=(16, 5), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this tuple when loading our data in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.element_spec == train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.experimental.load('train', element_spec=train_ds.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train\n",
    "\n",
    "initialize the Bert model, load as a pretrained model from transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "bert.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the frame around Bert, \n",
    "\n",
    "* Two input layers (one for input IDs and one for attention mask).\n",
    "\n",
    "* A post-bert dropout layer to reduce the likelihood of overfitting and improve generalization.\n",
    "\n",
    "* Max pooling layer to convert the 3D tensors output by Bert to 2D.\n",
    "\n",
    "* Final output activations using softmax for outputting categorical probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# two input layers, ensure layer name variables match to dictionary keys in TF dataset\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# access the transformer model within our bert object using the bert attribute ( bert.bert instead of bert)\n",
    "embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access final activations (alread max-pooled) [1]\n",
    "# convert bert embeddings into 5 output classes\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define model, specifying input and output layers. freeze the Bert layer because Bert is already highly trained, and contains a huge number of parameters so will take a very long time to train further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         787456      ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 5)            5125        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,102,853\n",
      "Trainable params: 792,581\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# (optional) freeze bert layer\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model architecture is now setup, initialize training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in our training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16, 5)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.float64)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the training and validation sets\n",
    "train_ds = tf.data.experimental.load('train', element_spec=train_ds.element_spec)\n",
    "val_ds = tf.data.experimental.load('val', element_spec=val_ds.element_spec)\n",
    "\n",
    "# view the input format\n",
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/479 [==============================] - 15698s 33s/step - loss: 1.5720 - accuracy: 0.2744 - val_loss: 1.5418 - val_accuracy: 0.3056\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 1050). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('sentiment_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         787456      ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 5)            5125        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,102,853\n",
      "Trainable params: 792,581\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('sentiment_model')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making predictions we need to format data, which requires two steps:\n",
    "\n",
    "* Tokenizing the data using the `bert-base-cased` tokenizer.\n",
    "\n",
    "* Transforming the data into a dictionary containing 'input_ids' and 'attention_mask' tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def prep_data(text):\n",
    "    tokens = tokenizer.encode_plus(text, max_length=512,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_token_type_ids=False,\n",
    "                                   return_tensors='tf')\n",
    "    # tokenizer returns int32 tensors, we need to return float64, so we use tf.cast\n",
    "    return {'input_ids': tf.cast(tokens['input_ids'], tf.float64),\n",
    "            'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10810728, 0.19894856, 0.20835981, 0.2931542 , 0.19143008],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(prep_data(\"hello world\"))[0]\n",
    "\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId  \\\n",
       "0    156061        8545   \n",
       "1    156062        8545   \n",
       "2    156063        8545   \n",
       "3    156064        8545   \n",
       "4    156065        8545   \n",
       "\n",
       "                                                   Phrase  \n",
       "0  An intermittently pleasing but mostly routine effort .  \n",
       "1    An intermittently pleasing but mostly routine effort  \n",
       "2                                                      An  \n",
       "3       intermittently pleasing but mostly routine effort  \n",
       "4              intermittently pleasing but mostly routine  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_csv('test.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>156076</td>\n",
       "      <td>8546</td>\n",
       "      <td>Kidman is really the only thing that 's worth watching in Birthday Girl , a film by the stage-trained Jez Butterworth -LRB- Mojo -RRB- that serves as yet another example of the sad decline of British comedies in the post-Full Monty world .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>156154</td>\n",
       "      <td>8547</td>\n",
       "      <td>Once you get into its rhythm ... the movie becomes a heady experience .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>156178</td>\n",
       "      <td>8548</td>\n",
       "      <td>I kept wishing I was watching a documentary about the wartime Navajos and what they accomplished instead of all this specious Hollywood hoo-ha .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>156219</td>\n",
       "      <td>8549</td>\n",
       "      <td>Kinnear does n't aim for our sympathy , but rather delivers a performance of striking skill and depth .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PhraseId  SentenceId  \\\n",
       "0      156061        8545   \n",
       "15     156076        8546   \n",
       "93     156154        8547   \n",
       "117    156178        8548   \n",
       "158    156219        8549   \n",
       "\n",
       "                                                                                                                                                                                                                                              Phrase  \n",
       "0                                                                                                                                                                                             An intermittently pleasing but mostly routine effort .  \n",
       "15   Kidman is really the only thing that 's worth watching in Birthday Girl , a film by the stage-trained Jez Butterworth -LRB- Mojo -RRB- that serves as yet another example of the sad decline of British comedies in the post-Full Monty world .  \n",
       "93                                                                                                                                                                           Once you get into its rhythm ... the movie becomes a heady experience .  \n",
       "117                                                                                                 I kept wishing I was watching a documentary about the wartime Navajos and what they accomplished instead of all this specious Hollywood hoo-ha .  \n",
       "158                                                                                                                                          Kinnear does n't aim for our sympathy , but rather delivers a performance of striking skill and depth .  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=['SentenceId'], keep='first')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create and initialize new column - sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>156076</td>\n",
       "      <td>8546</td>\n",
       "      <td>Kidman is really the only thing that 's worth watching in Birthday Girl , a film by the stage-trained Jez Butterworth -LRB- Mojo -RRB- that serves as yet another example of the sad decline of British comedies in the post-Full Monty world .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>156154</td>\n",
       "      <td>8547</td>\n",
       "      <td>Once you get into its rhythm ... the movie becomes a heady experience .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>156178</td>\n",
       "      <td>8548</td>\n",
       "      <td>I kept wishing I was watching a documentary about the wartime Navajos and what they accomplished instead of all this specious Hollywood hoo-ha .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>156219</td>\n",
       "      <td>8549</td>\n",
       "      <td>Kinnear does n't aim for our sympathy , but rather delivers a performance of striking skill and depth .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PhraseId  SentenceId  \\\n",
       "0      156061        8545   \n",
       "15     156076        8546   \n",
       "93     156154        8547   \n",
       "117    156178        8548   \n",
       "158    156219        8549   \n",
       "\n",
       "                                                                                                                                                                                                                                              Phrase  \\\n",
       "0                                                                                                                                                                                             An intermittently pleasing but mostly routine effort .   \n",
       "15   Kidman is really the only thing that 's worth watching in Birthday Girl , a film by the stage-trained Jez Butterworth -LRB- Mojo -RRB- that serves as yet another example of the sad decline of British comedies in the post-Full Monty world .   \n",
       "93                                                                                                                                                                           Once you get into its rhythm ... the movie becomes a heady experience .   \n",
       "117                                                                                                 I kept wishing I was watching a documentary about the wartime Navajos and what they accomplished instead of all this specious Hollywood hoo-ha .   \n",
       "158                                                                                                                                          Kinnear does n't aim for our sympathy , but rather delivers a performance of striking skill and depth .   \n",
       "\n",
       "    Sentiment  \n",
       "0           3  \n",
       "15          3  \n",
       "93          3  \n",
       "117         1  \n",
       "158         3  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'] = None\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # get token tensors\n",
    "    tokens = prep_data(row['Phrase'])\n",
    "    # get probabilities\n",
    "    probs = model.predict(tokens)\n",
    "    # find argmax for winning class\n",
    "    pred = np.argmax(probs)\n",
    "    # add to dataframe\n",
    "    df.at[i, 'Sentiment'] = pred\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66220</th>\n",
       "      <td>222281</td>\n",
       "      <td>11851</td>\n",
       "      <td>Not sweet enough to liven up its predictable story and will leave even fans of hip-hop sorely disappointed .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66239</th>\n",
       "      <td>222300</td>\n",
       "      <td>11852</td>\n",
       "      <td>Nasty , ugly , pointless and depressing , even if you hate clowns .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66253</th>\n",
       "      <td>222314</td>\n",
       "      <td>11853</td>\n",
       "      <td>With tightly organized efficiency , numerous flashbacks and a constant edge of tension , Miller 's film is one of 2002 's involvingly adult surprises .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66280</th>\n",
       "      <td>222341</td>\n",
       "      <td>11854</td>\n",
       "      <td>They should have called it Gutterball .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66287</th>\n",
       "      <td>222348</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PhraseId  SentenceId  \\\n",
       "66220    222281       11851   \n",
       "66239    222300       11852   \n",
       "66253    222314       11853   \n",
       "66280    222341       11854   \n",
       "66287    222348       11855   \n",
       "\n",
       "                                                                                                                                                        Phrase  \\\n",
       "66220                                             Not sweet enough to liven up its predictable story and will leave even fans of hip-hop sorely disappointed .   \n",
       "66239                                                                                      Nasty , ugly , pointless and depressing , even if you hate clowns .   \n",
       "66253  With tightly organized efficiency , numerous flashbacks and a constant edge of tension , Miller 's film is one of 2002 's involvingly adult surprises .   \n",
       "66280                                                                                                                  They should have called it Gutterball .   \n",
       "66287                                                                                                                   A long-winded , predictable scenario .   \n",
       "\n",
       "      Sentiment  \n",
       "66220         3  \n",
       "66239         1  \n",
       "66253         3  \n",
       "66280         1  \n",
       "66287         3  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
